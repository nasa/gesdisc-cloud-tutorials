{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-Cloud Data Access Clinic (June 2024)\n",
    "\n",
    "<p></p>\n",
    "\n",
    "<div style=\"background:#fc9090;border:1px solid #cccccc;padding:5px 10px;\"><big><b>Warning:  </b>Because this notebook uses the S3 protocol, <em><strong>it will only run in an environment with <a href=\"https://disc.gsfc.nasa.gov/information/glossary?title=AWS%20region\">us-west-2 AWS access</a></strong></em>.</big></div>\n",
    "\n",
    "### Overview\n",
    "\n",
    "This notebook contains steps for searching, accessing, subsetting, and averaging GPM_3IMERGHH granules using Python and in-cloud methods. It demonstrates how to search for S3 URLs using the python_cmr and earthaccess libraries, opening them with the earthaccess library, and finally, searching and accessing the GPM_3IMERGHH_precipitationCal Zarr store using the S3FS library.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "This notebook was written using Python 3.9, and requires these libraries and files: \n",
    "- netrc file with valid Earthdata Login credentials.\n",
    "- Xarray\n",
    "- python_cmr\n",
    "- earthaccess\n",
    "- requests\n",
    "- S3FS\n",
    "- NumPy\n",
    "\n",
    "### Other Links:\n",
    "- [Earthdata Webinar: Analyzing Precipitation Extremes Using Cloud Computing (Zarr)](https://www.earthdata.nasa.gov/learn/webinars-and-tutorials/ges-disc-30-10-2023)\n",
    "- [`earthaccess` Library How-tos](https://earthaccess.readthedocs.io/en/latest/howto/access-data/)\n",
    "- [Workshop Slides](https://docs.google.com/presentation/d/129rErCamSIO1bK0A_112pqPmvV82O9Z-a2LJLSRYtIY/edit?usp=sharing)\n",
    "\n",
    "### 1. Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import s3fs\n",
    "import earthaccess\n",
    "import numpy as np\n",
    "from cmr import VariableQuery\n",
    "import requests\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Authenticate with `earthaccess`\n",
    "\n",
    "The `earthaccess` library contains methods to automatically generate the .netrc file required for accessing S3 buckets, if it is not already present, using the `strategy=\"interactive\"` parameter. If the .netrc file does not exist, it will prompt you; if it does exist, or after it is created, it will automatically authenticate your S3 access with a token that will expire in one hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Search and Access Granules Directly from the S3 Bucket using `earthaccess`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once granule URLs are found, `earthaccess` will use its `open()` method to automatically format the object storage and authenticate so that we can access it. Then, we can open all the granules using `xr.open_mfdataset()`, while remembering to pass `group=Grid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Only open one day-this takes quite some time!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Subset over Northern California for February 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = [-124.295,38.954,-119.989,42.03]\n",
    "lon_slice = slice(bbox[0], bbox[2])\n",
    "lat_slice = slice(bbox[1], bbox[3])\n",
    "year = 2020\n",
    "\n",
    "start_time = f\"{year}-02-01T00:00:00\"\n",
    "end_time = f\"{year}-02-29T23:30:00\"\n",
    "time_slice = slice(start_time, end_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Calculate Monthly Mean and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Check Direct/External Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files directly from S3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files directly from on-premises HTTPS, or from the cloud, if available\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Access S3 Zarr Store Using `python-cmr` and `Xarray`\n",
    "\n",
    "Below, we create functions for authentication and opening the Zarr store, which is basically inside of another S3 bucket, and uses a different credential endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_credentials():\n",
    "    \"\"\"Makes the Oauth calls to authenticate with EDS and return a set of s3 same-region, read-only credentials.\"\"\"\n",
    "    response = requests.get(\"https://api.giovanni.earthdata.nasa.gov/s3credentials\")\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "def open_zarr_store(bucket_path):\n",
    "    creds = retrieve_credentials()\n",
    "\n",
    "    s3 = s3fs.S3FileSystem(key=creds[\"AccessKeyId\"], \n",
    "                           secret=creds[\"SecretAccessKey\"], \n",
    "                           token=creds[\"SessionToken\"])\n",
    "\n",
    "    store = s3.get_mapper(bucket_path)\n",
    "    return store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Search for Zarr Stores using `python_cmr`\n",
    "\n",
    "Here, we will use the `python_cmr` library to search for Zarr stores by variable, provider, and collection. Note that we have to manually parse out the collection alongside the variable, as there are multiple collections with `precipitationCal` as a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = \"precipitationCal\"\n",
    "provider = \"GES_DISC\"\n",
    "collection = \"GPM_3IMERGHH_06\"\n",
    "\n",
    "zarr_stores = [var for var in VariableQuery().provider(provider).get_all() if \"instance_information\" in var]\n",
    "\n",
    "for item in zarr_stores:\n",
    "    if collection in item.get('native_id', ''):\n",
    "        zarr_store_path = item['instance_information']['url']\n",
    "\n",
    "zarr_store_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Access Zarr Store using `xarray`\n",
    "\n",
    "Remember, this is only one variable! You will also notice that it is referred to as `variable`, and not `precipitationCal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "\n",
    "# Mask fill values. This is required until this bug is fixed\n",
    "ds_masked_dropped = ds_zarr.where(ds_zarr[\"time\"] != ds_zarr[\"time\"]._FillValue, drop=True)\n",
    "ds_masked_dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Subset over Northern California in February 2020\n",
    "\n",
    "Note that the latitude and longitude coordinates are now \"latitude\" and \"longitude\", instead of \"lat\" and \"lon\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = [-124.295,38.954,-119.989,42.03]\n",
    "lon_slice = slice(bbox[0], bbox[2])\n",
    "lat_slice = slice(bbox[1], bbox[3])\n",
    "year = 2020\n",
    "\n",
    "start_time = f\"{year}-02-01T00:00:00\"\n",
    "end_time = f\"{year}-02-28T23:30:00\" if year % 4 != 0 else f\"{year}-02-29T23:30:00\" # Handling leap years\n",
    "time_slice = slice(start_time, end_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, calculate the mean, and plot in a much faster duration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
